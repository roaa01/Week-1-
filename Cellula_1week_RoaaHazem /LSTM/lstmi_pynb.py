# -*- coding: utf-8 -*-
"""LSTMi.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/117NwQdPjmK4QJFwOHFBB8bvXJDh685QY
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')
nltk.download('punkt_tab')

# Load the dataset
df = pd.read_csv('cellula toxic data  (1).csv')

texts = df['query'].values
labels = df['Toxic Category'].values

# Label encoding
le = LabelEncoder()
labels_encoded = le.fit_transform(labels)
num_classes = len(le.classes_)

# Preprocessing functions (your code)
def remove_html_tags(text):
    pattern = re.compile(r'<.*?>')
    return pattern.sub(r'', text)

def remove_url(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    return pattern.sub(r'', text)

punc = string.punctuation

def remove_punc(text):
    return text.translate(str.maketrans('', '', punc))

chat_words = {
    "AFAIK": "As Far As I Know",
    "AFK": "Away From Keyboard",
    "ASAP": "As Soon As Possible",
    "ATK": "At The Keyboard",
    "ATM": "At The Moment",
    "A3": "Anytime, Anywhere, Anyplace",
    "BAK": "Back At Keyboard",
    "BBL": "Be Back Later",
    "BBS": "Be Back Soon",
    "BFN": "Bye For Now",
    "B4N": "Bye For Now",
    "BRB": "Be Right Back",
    "BRT": "Be Right There",
    "BTW": "By The Way",
    "B4": "Before",
    "B4N": "Bye For Now",
    "CU": "See You",
    "CUL8R": "See You Later",
    "CYA": "See You",
    "FAQ": "Frequently Asked Questions",
    "FC": "Fingers Crossed",
    "FWIW": "For What It's Worth",
    "FYI": "For Your Information",
    "GAL": "Get A Life",
    "GG": "Good Game",
    "GN": "Good Night",
    "GMTA": "Great Minds Think Alike",
    "GR8": "Great!",
    "G9": "Genius",
    "IC": "I See",
    "ICQ": "I Seek you (also a chat program)",
    "ILU": "ILU: I Love You",
    "IMHO": "In My Honest/Humble Opinion",
    "IMO": "In My Opinion",
    "IOW": "In Other Words",
    "IRL": "In Real Life",
    "KISS": "Keep It Simple, Stupid",
    "LDR": "Long Distance Relationship",
    "LMAO": "Laugh My A.. Off",
    "LOL": "Laughing Out Loud",
    "LTNS": "Long Time No See",
    "L8R": "Later",
    "MTE": "My Thoughts Exactly",
    "M8": "Mate",
    "NRN": "No Reply Necessary",
    "OIC": "Oh I See",
    "PITA": "Pain In The A..",
    "PRT": "Party",
    "PRW": "Parents Are Watching",
    "QPSA?": "Que Pasa?",
    "ROFL": "Rolling On The Floor Laughing",
    "ROFLOL": "Rolling On The Floor Laughing Out Loud",
    "ROTFLMAO": "Rolling On The Floor Laughing My A.. Off",
    "SK8": "Skate",
    "STATS": "Your sex and age",
    "ASL": "Age, Sex, Location",
    "THX": "Thank You",
    "TTFN": "Ta-Ta For Now!",
    "TTYL": "Talk To You Later",
    "U": "You",
    "U2": "You Too",
    "U4E": "Yours For Ever",
    "WB": "Welcome Back",
    "WTF": "What The F...",
    "WTG": "Way To Go!",
    "WUF": "Where Are You From?",
    "W8": "Wait...",
    "7K": "Sick:-D Laugher",
    "TFW": "That feeling when",
    "MFW": "My face when",
    "MRW": "My reaction when",
    "IFYP": "I feel your pain",
    "TNTL": "Trying not to laugh",
    "JK": "Just kidding",
    "IDC": "I don't care",
    "ILY": "I love you",
    "IMU": "I miss you",
    "ADIH": "Another day in hell",
    "ZZZ": "Sleeping, bored, tired",
    "WYWH": "Wish you were here",
    "TIME": "Tears in my eyes",
    "BAE": "Before anyone else",
    "FIMH": "Forever in my heart",
    "BSAAW": "Big smile and a wink",
    "BWL": "Bursting with laughter",
    "BFF": "Best friends forever",
    "CSL": "Can't stop laughing"
}

def chat_conversion(text):
    new_text = []
    for i in text.split():
        if i.upper() in chat_words:
            new_text.append(chat_words[i.upper()])
        else:
            new_text.append(i)
    return " ".join(new_text)

stopword = stopwords.words('english')

def remove_stopwords(text):
    new_text = []

    for word in text.split():
        if word in stopword:
            new_text.append('')
        else:
            new_text.append(word)
    x = new_text[:]
    new_text.clear()
    return " ".join(x)

wordnet_lemmatizer = WordNetLemmatizer()

def lem_words(word_list):
    return [wordnet_lemmatizer.lemmatize(word) for word in word_list]

def preprocess_text(text):
    text = remove_html_tags(text)
    text = remove_url(text)
    text = remove_punc(text)
    text = chat_conversion(text)
    text = remove_stopwords(text)
    text = word_tokenize(text)
    text = lem_words(text)
    return " ".join(text)

# Apply preprocessing (fixed for numpy array)
texts = [preprocess_text(text) for text in texts]

# Build vocabulary
all_words = ' '.join(texts).split()
word_counts = Counter(all_words)
vocab = sorted(word_counts, key=word_counts.get, reverse=True)
vocab_to_int = {word: ii + 1 for ii, word in enumerate(vocab)}

# Convert texts to sequences
texts_int = []
for text in texts:
    seq = [vocab_to_int[word] for word in text.split() if word in vocab_to_int]
    texts_int.append(seq)

# Pad sequences (handle empty sequences)
texts_tensor = [torch.tensor(seq) for seq in texts_int if seq]  # skip empty
padded_texts = pad_sequence(texts_tensor, batch_first=True, padding_value=0)

# Filter labels for non-empty sequences
labels_encoded = np.array([labels_encoded[i] for i in range(len(texts_int)) if texts_int[i]])

# Split data
X_train, X_test, y_train, y_test = train_test_split(padded_texts.numpy(), labels_encoded, test_size=0.2, random_state=42, stratify=labels_encoded)

X_train = torch.from_numpy(X_train).long()
X_test = torch.from_numpy(X_test).long()
y_train = torch.from_numpy(y_train).long()
y_test = torch.from_numpy(y_test).long()

# Dataset
class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.texts[idx], self.labels[idx]

# Dataloaders
train_ds = TextDataset(X_train, y_train)
test_ds = TextDataset(X_test, y_test)

batch_size = 32
train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
test_dl = DataLoader(test_ds, batch_size=batch_size)

# Model
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x):
        embedded = self.embedding(x)
        _, (hidden, _) = self.lstm(embedded)
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        out = self.fc(hidden)
        return out

vocab_size = len(vocab_to_int) + 1
embedding_dim = 128
hidden_dim = 256
model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_classes)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training
epochs = 20
train_losses = []
for epoch in range(epochs):
    model.train()
    epoch_loss = 0
    num_batches = 0
    for texts_batch, labels_batch in train_dl:
        optimizer.zero_grad()
        output = model(texts_batch)
        loss = criterion(output, labels_batch)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        num_batches += 1
    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0
    train_losses.append(avg_loss)
    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')

# Evaluation
model.eval()
preds = []
true = []
with torch.no_grad():
    for texts_batch, labels_batch in test_dl:
        output = model(texts_batch)
        pred = torch.argmax(output, dim=1)
        preds.extend(pred.cpu().numpy())
        true.extend(labels_batch.cpu().numpy())

f1 = f1_score(true, preds, average='weighted')
print(f'F1 Score: {f1:.4f}')

# Confusion Matrix
cm = confusion_matrix(true, preds)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.pdf')

# Training Loss Graph
plt.figure(figsize=(8, 6))
plt.plot(range(1, epochs+1), train_losses, marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss over Epochs')
plt.savefig('training_loss.pdf')